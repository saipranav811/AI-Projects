{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab9b6a64",
   "metadata": {},
   "source": [
    "\n",
    "# TEJ SAI PRANAV REDDY KAGITALA - TXK220023\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad8ca5",
   "metadata": {},
   "source": [
    "# General accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f20d4b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Severity       StartLat       StartLng       Distance  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean        2.338270      36.514226     -95.918245       0.288088   \n",
      "std         0.551132       4.895905      17.357249       1.653184   \n",
      "min         1.000000      24.569300    -124.474380       0.000000   \n",
      "25%         2.000000      33.622412    -117.488407       0.000000   \n",
      "50%         2.000000      35.840891     -91.074997       0.000000   \n",
      "75%         3.000000      40.300162     -80.995310       0.010000   \n",
      "max         4.000000      48.964230     -68.368760     176.279999   \n",
      "\n",
      "             Zipcode    Temperature       Humidity       Pressure  \\\n",
      "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
      "mean    57969.864520      61.727966      64.828860      29.709953   \n",
      "std     30456.072213      18.970448      23.154256       1.324756   \n",
      "min      1001.000000     -27.900000       0.000000       0.000000   \n",
      "25%     29704.000000      50.000000      48.000000      29.730000   \n",
      "50%     60148.000000      64.000000      67.000000      29.950000   \n",
      "75%     90265.000000      75.900000      84.000000      30.090000   \n",
      "max     99338.000000     140.000000     100.000000      30.920000   \n",
      "\n",
      "          Visibility      WindSpeed  Precipitation  \n",
      "count  100000.000000  100000.000000  100000.000000  \n",
      "mean        9.100749       7.302313       0.006801  \n",
      "std         2.914160       6.208804       0.131558  \n",
      "min         0.000000       0.000000       0.000000  \n",
      "25%        10.000000       3.500000       0.000000  \n",
      "50%        10.000000       6.900000       0.000000  \n",
      "75%        10.000000      10.400000       0.000000  \n",
      "max        80.000000     822.800000      24.000000  \n",
      "Severity            0\n",
      "StartTime           0\n",
      "EndTime             0\n",
      "StartLat            0\n",
      "StartLng            0\n",
      "Distance            0\n",
      "Street              0\n",
      "City                0\n",
      "State               0\n",
      "Zipcode             0\n",
      "Temperature         0\n",
      "Humidity            0\n",
      "Pressure            0\n",
      "Visibility          0\n",
      "WindSpeed           0\n",
      "Precipitation       0\n",
      "WeatherCondition    0\n",
      "Amenity             0\n",
      "Bump                0\n",
      "Crossing            0\n",
      "GiveWay             0\n",
      "Junction            0\n",
      "NoExit              0\n",
      "Railway             0\n",
      "Roundabout          0\n",
      "Station             0\n",
      "Stop                0\n",
      "TrafficCalming      0\n",
      "TrafficSignal       0\n",
      "dtype: int64\n",
      "2    67708\n",
      "3    28297\n",
      "4     3175\n",
      "1      820\n",
      "Name: Severity, dtype: int64\n",
      "Ideal number of components with CEV >= 0.85: 3\n",
      "Accuracy score with 1 components: 0.6789571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEJ SAI PRANAV REDDY\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score with 5 components: 0.6789428571428572\n",
      "Accuracy score with 10 components: 0.6801571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEJ SAI PRANAV REDDY\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv('general_accidents.csv')\n",
    "\n",
    "# 1. Initial data exploration and analysis\n",
    "print(df.describe()) # Basic statistics summary for all columns\n",
    "print(df.isnull().sum()) # Check for missing values\n",
    "print(df['Severity'].value_counts()) # Count of different values in the Severity column\n",
    "\n",
    "# 2. Create the PCA model, separate train and test data (30% Train, 70% Test)\n",
    "X = df.drop(['Severity', 'StartTime', 'EndTime', 'Street', 'City', 'State', 'Zipcode', 'WeatherCondition'], axis=1)\n",
    "y = df['Severity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "\n",
    "# 3. Train the model\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# 4. Find the ideal number of components to be used so you can have a C.E.V. >= .85\n",
    "n_components = 1\n",
    "cev = 0.0\n",
    "while cev < 0.85:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_train)\n",
    "    cev = sum(pca.explained_variance_ratio_)\n",
    "    n_components += 1\n",
    "\n",
    "print(f\"Ideal number of components with CEV >= 0.85: {n_components-1}\")\n",
    "\n",
    "# 5. Use the test data and calculate the accuracy for 3 different number of components, showing improvement on the results as you approach a higher C.E.V.\n",
    "for n in [1, 5, 10]:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Train a simple logistic regression model\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=42).fit(X_train_pca, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    print(f\"Accuracy score with {n} components: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda5b070",
   "metadata": {},
   "source": [
    "# Movie ratings dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18eb4ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Action     Adventure     Animation      Children        Comedy  \\\n",
      "count  1.000209e+06  1.000209e+06  1.000209e+06  1.000209e+06  1.000209e+06   \n",
      "mean   2.574032e-01  1.339250e-01  4.328395e-02  7.217092e-02  3.565055e-01   \n",
      "std    4.372036e-01  3.405719e-01  2.034957e-01  2.587708e-01  4.789672e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    1.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  1.000000e+00   \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "              Crime   Documentary         Drama       Fantasy     Film-Noir  \\\n",
      "count  1.000209e+06  1.000209e+06  1.000209e+06  1.000209e+06  1.000209e+06   \n",
      "mean   7.952438e-02  7.908347e-03  3.544549e-01  3.629341e-02  1.825718e-02   \n",
      "std    2.705556e-01  8.857659e-02  4.783481e-01  1.870194e-01  1.338801e-01   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "50%    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "75%    0.000000e+00  0.000000e+00  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "max    1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
      "\n",
      "       ...        Sci-Fi      Thriller    Unknown           War       Western  \\\n",
      "count  ...  1.000209e+06  1.000209e+06  1000209.0  1.000209e+06  1.000209e+06   \n",
      "mean   ...  1.572611e-01  1.896404e-01        0.0  6.851268e-02  2.067868e-02   \n",
      "std    ...  3.640470e-01  3.920166e-01        0.0  2.526237e-01  1.423063e-01   \n",
      "min    ...  0.000000e+00  0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
      "25%    ...  0.000000e+00  0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
      "50%    ...  0.000000e+00  0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
      "75%    ...  0.000000e+00  0.000000e+00        0.0  0.000000e+00  0.000000e+00   \n",
      "max    ...  1.000000e+00  1.000000e+00        0.0  1.000000e+00  1.000000e+00   \n",
      "\n",
      "       (no genres listed)           Age        Gender    Occupation  \\\n",
      "count           1000209.0  1.000209e+06  1.000209e+06  1.000209e+06   \n",
      "mean                  0.0  2.973831e+01  7.536115e-01  1.105189e+01   \n",
      "std                   0.0  1.175198e+01  4.309076e-01  6.571510e+00   \n",
      "min                   0.0  1.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%                   0.0  2.500000e+01  1.000000e+00  6.000000e+00   \n",
      "50%                   0.0  2.500000e+01  1.000000e+00  1.200000e+01   \n",
      "75%                   0.0  3.500000e+01  1.000000e+00  1.700000e+01   \n",
      "max                   0.0  5.600000e+01  1.000000e+00  2.100000e+01   \n",
      "\n",
      "             Rating  \n",
      "count  1.000209e+06  \n",
      "mean   3.581564e+00  \n",
      "std    1.117102e+00  \n",
      "min    1.000000e+00  \n",
      "25%    3.000000e+00  \n",
      "50%    4.000000e+00  \n",
      "75%    4.000000e+00  \n",
      "max    5.000000e+00  \n",
      "\n",
      "[8 rows x 25 columns]\n",
      "Movie ID              0\n",
      "Action                0\n",
      "Adventure             0\n",
      "Animation             0\n",
      "Children              0\n",
      "Comedy                0\n",
      "Crime                 0\n",
      "Documentary           0\n",
      "Drama                 0\n",
      "Fantasy               0\n",
      "Film-Noir             0\n",
      "Horror                0\n",
      "IMAX                  0\n",
      "Musical               0\n",
      "Mystery               0\n",
      "Romance               0\n",
      "Sci-Fi                0\n",
      "Thriller              0\n",
      "Unknown               0\n",
      "War                   0\n",
      "Western               0\n",
      "(no genres listed)    0\n",
      "User ID               0\n",
      "Age                   0\n",
      "Gender                0\n",
      "Occupation            0\n",
      "Zipcode               0\n",
      "Rating                0\n",
      "dtype: int64\n",
      "4    348971\n",
      "3    261197\n",
      "5    226310\n",
      "2    107557\n",
      "1     56174\n",
      "Name: Rating, dtype: int64\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Title', 'MovieID', 'UserID'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16300\\2429913320.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 2. Create the PCA model, separate train and test data (30% Train, 70% Test)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Rating'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MovieID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'UserID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Age'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Gender'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Occupation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Zipcode'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4955\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4956\u001b[0m         \"\"\"\n\u001b[1;32m-> 4957\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   4958\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4959\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4265\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4266\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4267\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[0;32m   4309\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4310\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4311\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4312\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6659\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6660\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6661\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{list(labels[mask])} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6662\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6663\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Title', 'MovieID', 'UserID'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset into a Pandas dataframe\n",
    "df = pd.read_csv('movies_ratings.csv')\n",
    "\n",
    "# 1. Initial data exploration and analysis\n",
    "print(df.describe()) # Basic statistics summary for all columns\n",
    "print(df.isnull().sum()) # Check for missing values\n",
    "print(df['Rating'].value_counts()) # Count of different values in the rating column\n",
    "\n",
    "# 2. Create the PCA model, separate train and test data (30% Train, 70% Test)\n",
    "X = df.drop(['Title', 'Rating', 'MovieID', 'UserID', 'Age', 'Gender', 'Occupation', 'Zipcode'], axis=1)\n",
    "y = df['Rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "\n",
    "# 3. Train the model\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "\n",
    "# 4. Find the ideal number of components to be used so you can have a C.E.V. >= .85\n",
    "n_components = 1\n",
    "cev = 0.0\n",
    "while cev < 0.85:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_train)\n",
    "    cev = sum(pca.explained_variance_ratio_)\n",
    "    n_components += 1\n",
    "\n",
    "print(f\"Ideal number of components with CEV >= 0.85: {n_components-1}\")\n",
    "\n",
    "# 5. Use the test data and calculate the accuracy for 3 different number of components, showing improvement on the results as you approach a higher C.E.V.\n",
    "for n in [1, 5, 10]:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Train a simple logistic regression model\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(random_state=42).fit(X_train_pca, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test_pca)\n",
    "    print(f\"Accuracy score with {n} components: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff72584",
   "metadata": {},
   "source": [
    "# music popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ba8425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_id                    0\n",
      "track_name                  5\n",
      "track_artist                5\n",
      "track_popularity            0\n",
      "track_album_id              0\n",
      "track_album_name            5\n",
      "track_album_release_date    0\n",
      "playlist_name               0\n",
      "playlist_id                 0\n",
      "playlist_genre              0\n",
      "playlist_subgenre           0\n",
      "danceability                0\n",
      "energy                      0\n",
      "key                         0\n",
      "loudness                    0\n",
      "mode                        0\n",
      "speechiness                 0\n",
      "acousticness                0\n",
      "instrumentalness            0\n",
      "liveness                    0\n",
      "valence                     0\n",
      "tempo                       0\n",
      "duration_ms                 0\n",
      "dtype: int64\n",
      "track_id                     object\n",
      "track_name                   object\n",
      "track_artist                 object\n",
      "track_popularity              int64\n",
      "track_album_id               object\n",
      "track_album_name             object\n",
      "track_album_release_date     object\n",
      "playlist_name                object\n",
      "playlist_id                  object\n",
      "playlist_genre               object\n",
      "playlist_subgenre            object\n",
      "danceability                float64\n",
      "energy                      float64\n",
      "key                           int64\n",
      "loudness                    float64\n",
      "mode                          int64\n",
      "speechiness                 float64\n",
      "acousticness                float64\n",
      "instrumentalness            float64\n",
      "liveness                    float64\n",
      "valence                     float64\n",
      "tempo                       float64\n",
      "duration_ms                   int64\n",
      "dtype: object\n",
      "       track_popularity  danceability        energy           key  \\\n",
      "count      32833.000000  32833.000000  32833.000000  32833.000000   \n",
      "mean          42.477081      0.654850      0.698619      5.374471   \n",
      "std           24.984074      0.145085      0.180910      3.611657   \n",
      "min            0.000000      0.000000      0.000175      0.000000   \n",
      "25%           24.000000      0.563000      0.581000      2.000000   \n",
      "50%           45.000000      0.672000      0.721000      6.000000   \n",
      "75%           62.000000      0.761000      0.840000      9.000000   \n",
      "max          100.000000      0.983000      1.000000     11.000000   \n",
      "\n",
      "           loudness          mode   speechiness  acousticness  \\\n",
      "count  32833.000000  32833.000000  32833.000000  32833.000000   \n",
      "mean      -6.719499      0.565711      0.107068      0.175334   \n",
      "std        2.988436      0.495671      0.101314      0.219633   \n",
      "min      -46.448000      0.000000      0.000000      0.000000   \n",
      "25%       -8.171000      0.000000      0.041000      0.015100   \n",
      "50%       -6.166000      1.000000      0.062500      0.080400   \n",
      "75%       -4.645000      1.000000      0.132000      0.255000   \n",
      "max        1.275000      1.000000      0.918000      0.994000   \n",
      "\n",
      "       instrumentalness      liveness       valence         tempo  \\\n",
      "count      32833.000000  32833.000000  32833.000000  32833.000000   \n",
      "mean           0.084747      0.190176      0.510561    120.881132   \n",
      "std            0.224230      0.154317      0.233146     26.903624   \n",
      "min            0.000000      0.000000      0.000000      0.000000   \n",
      "25%            0.000000      0.092700      0.331000     99.960000   \n",
      "50%            0.000016      0.127000      0.512000    121.984000   \n",
      "75%            0.004830      0.248000      0.693000    133.918000   \n",
      "max            0.994000      0.996000      0.991000    239.440000   \n",
      "\n",
      "         duration_ms  \n",
      "count   32833.000000  \n",
      "mean   225799.811622  \n",
      "std     59834.006182  \n",
      "min      4000.000000  \n",
      "25%    187819.000000  \n",
      "50%    216000.000000  \n",
      "75%    253585.000000  \n",
      "max    517810.000000  \n",
      "                  track_popularity  danceability    energy       key  \\\n",
      "track_popularity          1.000000      0.064748 -0.109112 -0.000650   \n",
      "danceability              0.064748      1.000000 -0.086073  0.011736   \n",
      "energy                   -0.109112     -0.086073  1.000000  0.010052   \n",
      "key                      -0.000650      0.011736  0.010052  1.000000   \n",
      "loudness                  0.057687      0.025335  0.676625  0.000959   \n",
      "mode                      0.010637     -0.058647 -0.004800 -0.174093   \n",
      "speechiness               0.006819      0.181721 -0.032150  0.022607   \n",
      "acousticness              0.085159     -0.024519 -0.539745  0.004306   \n",
      "instrumentalness         -0.149872     -0.008655  0.033247  0.005968   \n",
      "liveness                 -0.054584     -0.123859  0.161223  0.002887   \n",
      "valence                   0.033231      0.330523  0.151103  0.019914   \n",
      "tempo                    -0.005378     -0.184084  0.149951 -0.013370   \n",
      "duration_ms              -0.143682     -0.096879  0.012611  0.015139   \n",
      "\n",
      "                  loudness      mode  speechiness  acousticness  \\\n",
      "track_popularity  0.057687  0.010637     0.006819      0.085159   \n",
      "danceability      0.025335 -0.058647     0.181721     -0.024519   \n",
      "energy            0.676625 -0.004800    -0.032150     -0.539745   \n",
      "key               0.000959 -0.174093     0.022607      0.004306   \n",
      "loudness          1.000000 -0.019289     0.010339     -0.361638   \n",
      "mode             -0.019289  1.000000    -0.063512      0.009415   \n",
      "speechiness       0.010339 -0.063512     1.000000      0.026092   \n",
      "acousticness     -0.361638  0.009415     0.026092      1.000000   \n",
      "instrumentalness -0.147824 -0.006741    -0.103424     -0.006850   \n",
      "liveness          0.077613 -0.005549     0.055426     -0.077243   \n",
      "valence           0.053384  0.002614     0.064659     -0.016845   \n",
      "tempo             0.093767  0.014329     0.044603     -0.112724   \n",
      "duration_ms      -0.115058  0.015634    -0.089431     -0.081581   \n",
      "\n",
      "                  instrumentalness  liveness   valence     tempo  duration_ms  \n",
      "track_popularity         -0.149872 -0.054584  0.033231 -0.005378    -0.143682  \n",
      "danceability             -0.008655 -0.123859  0.330523 -0.184084    -0.096879  \n",
      "energy                    0.033247  0.161223  0.151103  0.149951     0.012611  \n",
      "key                       0.005968  0.002887  0.019914 -0.013370     0.015139  \n",
      "loudness                 -0.147824  0.077613  0.053384  0.093767    -0.115058  \n",
      "mode                     -0.006741 -0.005549  0.002614  0.014329     0.015634  \n",
      "speechiness              -0.103424  0.055426  0.064659  0.044603    -0.089431  \n",
      "acousticness             -0.006850 -0.077243 -0.016845 -0.112724    -0.081581  \n",
      "instrumentalness          1.000000 -0.005507 -0.175402  0.023335     0.063235  \n",
      "liveness                 -0.005507  1.000000 -0.020560  0.021018     0.006138  \n",
      "valence                  -0.175402 -0.020560  1.000000 -0.025732    -0.032225  \n",
      "tempo                     0.023335  0.021018 -0.025732  1.000000    -0.001412  \n",
      "duration_ms               0.063235  0.006138 -0.032225 -0.001412     1.000000  \n",
      "Ideal number of components: 9\n",
      "Cumulative explained variance ratio: 0.8851737268590125\n",
      "Number of components: 2\n",
      "R-squared score: 0.019355118962945395\n",
      "Number of components: 4\n",
      "R-squared score: 0.02971477930740074\n",
      "Number of components: 6\n",
      "R-squared score: 0.05221392009968395\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Perform the initial data exploration and analysis\n",
    "df = pd.read_csv('music_popularity.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the data types of columns\n",
    "print(df.dtypes)\n",
    "\n",
    "# Summary statistics of the dataset\n",
    "print(df.describe())\n",
    "\n",
    "# Correlation matrix of the dataset\n",
    "print(df.corr())\n",
    "\n",
    "# 2. Create the PCA model, separate train and test data (30% Train, 70% Test)\n",
    "X = df.drop(['track_id', 'track_name', 'track_artist', 'track_album_id', 'track_album_name', 'track_album_release_date', 'playlist_name', 'playlist_id', 'playlist_genre', 'playlist_subgenre', 'track_popularity'], axis=1)\n",
    "y = df['track_popularity']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# 3. Train the model\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "# 4. Find the ideal number of components to be used so you can have a C.E.V. >= .85\n",
    "n_components = X_train.shape[1]\n",
    "for i in range(1, n_components+1):\n",
    "    pca = PCA(n_components=i)\n",
    "    X_train_pca = pca.fit_transform(X_train_std)\n",
    "    cev = np.sum(pca.explained_variance_ratio_)\n",
    "    if cev >= 0.85:\n",
    "        print(\"Ideal number of components:\", i)\n",
    "        print(\"Cumulative explained variance ratio:\", cev)\n",
    "        break\n",
    "\n",
    "# 5. Use the test data and calculate the accuracy for 3 different number of components, showing improvement on the results as you approach a higher C.E.V.\n",
    "n_components_list = [2, 4, 6]\n",
    "for n in n_components_list:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_train_pca = pca.fit_transform(X_train_std)\n",
    "    X_test_pca = pca.transform(X_test_std)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_pca, y_train)\n",
    "    y_pred = lr.predict(X_test_pca)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(\"Number of components:\", n)\n",
    "    print(\"R-squared score:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac182529",
   "metadata": {},
   "source": [
    "# price of autos dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cdce3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   symboling         make fuel_type aspiration  num_of_doors   body_style  \\\n",
      "0          3  alfa-romero       gas        std             2  convertible   \n",
      "1          3  alfa-romero       gas        std             2  convertible   \n",
      "2          1  alfa-romero       gas        std             2    hatchback   \n",
      "3          2         audi       gas        std             4        sedan   \n",
      "4          2         audi       gas        std             4        sedan   \n",
      "\n",
      "  drive_wheels engine_location  wheel_base  length  ...  engine_size  \\\n",
      "0          rwd           front        88.6   168.8  ...          130   \n",
      "1          rwd           front        88.6   168.8  ...          130   \n",
      "2          rwd           front        94.5   171.2  ...          152   \n",
      "3          fwd           front        99.8   176.6  ...          109   \n",
      "4          4wd           front        99.4   176.6  ...          136   \n",
      "\n",
      "   fuel_system  bore stroke  compression_ratio  horsepower peak_rpm  city_mpg  \\\n",
      "0         mpfi  3.47   2.68                  9         111     5000        21   \n",
      "1         mpfi  3.47   2.68                  9         111     5000        21   \n",
      "2         mpfi  2.68   3.47                  9         154     5000        19   \n",
      "3         mpfi  3.19   3.40                 10         102     5500        24   \n",
      "4         mpfi  3.19   3.40                  8         115     5500        18   \n",
      "\n",
      "   highway_mpg  price  \n",
      "0           27  13495  \n",
      "1           27  16500  \n",
      "2           26  16500  \n",
      "3           30  13950  \n",
      "4           22  17450  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "        symboling  num_of_doors  wheel_base      length       width  \\\n",
      "count  193.000000    193.000000  193.000000  193.000000  193.000000   \n",
      "mean     0.797927      3.160622   98.923834  174.326425   65.893782   \n",
      "std      1.235582      0.989583    6.152409   12.478593    2.137795   \n",
      "min     -2.000000      2.000000   86.600000  141.100000   60.300000   \n",
      "25%      0.000000      2.000000   94.500000  166.300000   64.100000   \n",
      "50%      1.000000      4.000000   97.000000  173.200000   65.400000   \n",
      "75%      2.000000      4.000000  102.400000  184.600000   66.900000   \n",
      "max      3.000000      4.000000  120.900000  208.100000   72.000000   \n",
      "\n",
      "           height  curb_weight  num_of_cylinders  engine_size        bore  \\\n",
      "count  193.000000   193.000000        193.000000   193.000000  193.000000   \n",
      "mean    53.869948  2561.507772          4.419689   128.124352    3.330622   \n",
      "std      2.394770   526.700026          1.023182    41.590452    0.272385   \n",
      "min     47.800000  1488.000000          3.000000    61.000000    2.540000   \n",
      "25%     52.000000  2145.000000          4.000000    98.000000    3.150000   \n",
      "50%     54.100000  2414.000000          4.000000   120.000000    3.310000   \n",
      "75%     55.700000  2952.000000          4.000000   146.000000    3.590000   \n",
      "max     59.800000  4066.000000         12.000000   326.000000    3.940000   \n",
      "\n",
      "           stroke  compression_ratio  horsepower     peak_rpm    city_mpg  \\\n",
      "count  193.000000         193.000000  193.000000   193.000000  193.000000   \n",
      "mean     3.248860           9.860104  103.481865  5099.740933   25.326425   \n",
      "std      0.315421           4.002098   37.960107   468.694369    6.387828   \n",
      "min      2.070000           7.000000   48.000000  4150.000000   13.000000   \n",
      "25%      3.110000           8.000000   70.000000  4800.000000   19.000000   \n",
      "50%      3.290000           9.000000   95.000000  5100.000000   25.000000   \n",
      "75%      3.410000           9.000000  116.000000  5500.000000   30.000000   \n",
      "max      4.170000          23.000000  262.000000  6600.000000   49.000000   \n",
      "\n",
      "       highway_mpg         price  \n",
      "count   193.000000    193.000000  \n",
      "mean     30.787565  13285.025907  \n",
      "std       6.816910   8089.082886  \n",
      "min      16.000000   5118.000000  \n",
      "25%      25.000000   7738.000000  \n",
      "50%      30.000000  10245.000000  \n",
      "75%      34.000000  16515.000000  \n",
      "max      54.000000  45400.000000  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'bmw'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16300\\3637223112.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# 3. Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mX_train_pca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0mX_test_pca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mC\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[1;34m'np.ascontiguousarray'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \"\"\"\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    428\u001b[0m             )\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m         X = self._validate_data(\n\u001b[0m\u001b[0;32m    431\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2062\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2063\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2064\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2066\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'bmw'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('price_of_autos.csv')\n",
    "\n",
    "# 1. Perform the initial data exploration and analysis\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "\n",
    "# 2. Create the PCA model, separate train and test data (30% Train, 70% Test)\n",
    "X = df.drop(['price'], axis=1)\n",
    "y = df['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 3. Train the model\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "# 4. Find the ideal number of components to be used so you can have a C.E.V. >= .85\n",
    "cev = np.cumsum(pca.explained_variance_ratio_)\n",
    "n_components = np.argmax(cev >= 0.85) + 1\n",
    "print(f\"Ideal number of components: {n_components}\")\n",
    "\n",
    "# 5. Use the test data and calculate the accuracy for 3 different number of components,\n",
    "#    showing improvement on the results as you approach a higher C.E.V.\n",
    "for n in [1, 5, n_components]:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_pca, y_train)\n",
    "    y_pred = lr.predict(X_test_pca)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Number of components: {n}, R^2 score: {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
